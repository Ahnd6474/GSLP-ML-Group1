{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12471997,"sourceType":"datasetVersion","datasetId":7868514}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Roberta finetuning","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets torch scikit-learn\n!pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T11:03:47.976257Z","iopub.execute_input":"2025-07-15T11:03:47.976512Z","iopub.status.idle":"2025-07-15T11:03:54.980857Z","shell.execute_reply.started":"2025-07-15T11:03:47.976495Z","shell.execute_reply":"2025-07-15T11:03:54.980119Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/ai-text/ai_press_releases.csv')\ndf=df.dropna()\nhuman=df['non_chat_gpt_press_release']\nai=df['chat_gpt_generated_release']\nhu=[]\na=[]\nfor i in human:\n    l=list(i.split('. '))\n    hu.extend(l)\nfor i in ai:\n    l=list(i.split('. '))\n    a.extend(l)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:59:38.943007Z","iopub.execute_input":"2025-07-15T10:59:38.943591Z","iopub.status.idle":"2025-07-15T10:59:40.344488Z","shell.execute_reply.started":"2025-07-15T10:59:38.943565Z","shell.execute_reply":"2025-07-15T10:59:40.343953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(a))\nprint(len(hu))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T10:44:06.085924Z","iopub.execute_input":"2025-07-15T10:44:06.086513Z","iopub.status.idle":"2025-07-15T10:44:06.091118Z","shell.execute_reply.started":"2025-07-15T10:44:06.086484Z","shell.execute_reply":"2025-07-15T10:44:06.090249Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ap=a.copy()\na.extend(hu)\ntexts=a\nlabels=[0 if i<len(ap) else 1 for i in range(len(texts))]\nfrom sklearn.model_selection import train_test_split\n\n# 1) 먼저 train_temp(80%)와 test(20%) 분할\ntexts_train_val, texts_test, labels_train_val, labels_test = train_test_split(\n    texts,\n    labels,\n    test_size=0.2,       # 전체의 20%\n    random_state=42,\n    stratify=labels      # 레이블 비율 유지\n)\n\n# 2) train_temp을 다시 train(75% of temp → 60% 전체)와 val(25% of temp → 20% 전체)로 분할\ntexts_train, texts_val, labels_train, labels_val = train_test_split(\n    texts_train_val,\n    labels_train_val,\n    test_size=0.25,      # train_temp의 25% → 전체의 0.2\n    random_state=42,\n    stratify=labels_train_val\n)\n\nprint(f\"Train: {len(texts_train)} samples\")\nprint(f\"Valid: {len(texts_val)} samples\")\nprint(f\"Test : {len(texts_test)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T11:00:05.445358Z","iopub.execute_input":"2025-07-15T11:00:05.445928Z","iopub.status.idle":"2025-07-15T11:00:06.687943Z","shell.execute_reply.started":"2025-07-15T11:00:05.445902Z","shell.execute_reply":"2025-07-15T11:00:06.687260Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.optim import AdamW\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# — 데이터 & DataLoader 준비 (이미 정의된 texts_train/val/test, labels_* 목록 사용) —\ntokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n\ndef collate_fn(batch):\n    enc = tokenizer(\n        [x[\"text\"] for x in batch],\n        padding=\"longest\",\n        truncation=True,\n        max_length=256,\n        return_tensors=\"pt\"\n    )\n    enc[\"labels\"] = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n    return enc\n\ntrain_ds = Dataset.from_dict({\"text\": texts_train, \"label\": labels_train})\nval_ds   = Dataset.from_dict({\"text\": texts_val,   \"label\": labels_val})\ntest_ds  = Dataset.from_dict({\"text\": texts_test,  \"label\": labels_test})\n\ntrain_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\nval_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, collate_fn=collate_fn)\ntest_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n# — 모델/옵티마이저 세팅 —\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel  = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(device)\noptim  = AdamW(model.parameters(), lr=2e-5)\n\nnum_epochs = 8\n\nfor epoch in range(1, num_epochs+1):\n    # ── 1) TRAIN ───────────────────────────────────────────\n    model.train()\n    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} [TRAIN]\")\n    for batch in train_loop:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss    = outputs.loss\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n        gpu_mem = torch.cuda.memory_allocated(device) // (1024**2)\n        train_loop.set_postfix(loss=f\"{loss.item():.4f}\", gpu_mem=f\"{gpu_mem}MiB\")\n\n    # ── 2) VALIDATION ──────────────────────────────────────\n    model.eval()\n    all_preds, all_labels = [], []\n    val_loop = tqdm(val_loader, desc=f\"Epoch {epoch}/{num_epochs} [VAL]  \")\n    with torch.no_grad():\n        for batch in val_loop:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(**batch).logits\n            preds  = torch.argmax(logits, dim=-1).cpu().tolist()\n            labels = batch[\"labels\"].cpu().tolist()\n            all_preds += preds\n            all_labels += labels\n    val_acc = accuracy_score(all_labels, all_preds)\n    val_f1  = f1_score(all_labels, all_preds, average=\"weighted\")\n    print(f\"→ Validation | Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n\n    # ── 3) TEST ────────────────────────────────────────────\n    all_preds, all_labels = [], []\n    test_loop = tqdm(test_loader, desc=f\"Epoch {epoch}/{num_epochs} [TEST] \")\n    with torch.no_grad():\n        for batch in test_loop:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            logits = model(**batch).logits\n            preds  = torch.argmax(logits, dim=-1).cpu().tolist()\n            labels = batch[\"labels\"].cpu().tolist()\n            all_preds += preds\n            all_labels += labels\n    test_acc = accuracy_score(all_labels, all_preds)\n    test_f1  = f1_score(all_labels, all_preds, average=\"weighted\")\n    print(f\"→ Test       | Acc: {test_acc:.4f}, F1: {test_f1:.4f}\")\n\n    # ── 4) MODEL SAVE ──────────────────────────────────────\n    save_dir = f\"/kaggle/working/checkpoint-epoch{epoch}\"\n    model.save_pretrained(save_dir)\n    tokenizer.save_pretrained(save_dir)\n    print(f\"→ Model & Tokenizer saved to: {save_dir}\\n\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-15T11:33:47.017618Z","iopub.execute_input":"2025-07-15T11:33:47.018359Z","execution_failed":"2025-07-15T11:44:26.727Z"}},"outputs":[],"execution_count":null}]}